import argparse
import yaml
import copy
import time
import os
import torch
from torch.utils.tensorboard import SummaryWriter
from utils import set_seed
from ddpm.model import DDPMModel
from reward.model import RewardModel
from ddpm.schedule import get_beta_alpha_linear
from ppo.algorithm import *
from ppo.utils import *

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--cfg", default="configs/default.yaml")
    args = ap.parse_args()
    cfg = yaml.safe_load(open(args.cfg))

    set_seed(cfg["seed"])
    device = torch.device(cfg["device"] if torch.cuda.is_available() else "cpu")

    ddpm_cur = DDPMModel().to(device)
    ddpm_old = copy.deepcopy(ddpm_cur).to(device)
    for p in ddpm_old.parameters():
        p.requires_grad = False
    ddpm_old.eval()

    rm = RewardModel().to(device)
    if cfg["path"]["rm"]:
        rm_ckpt_path = cfg["path"]["rm"]
        rm_state = torch.load(rm_ckpt_path, map_location="cpu")
        rm.load_state_dict(rm_state["model_state_dict"] if "model_state_dict" in rm_state else rm_state)
    rm.eval()
    for p in rm.parameters():
        p.requires_grad = False

    _, alphas, alpha_bars = get_beta_alpha_linear()
    alphas = alphas.to(device)
    alpha_bars = alpha_bars.to(device)

    eps_to_mu_fn = lambda x_t, t_idx, eps_pred: compute_mu_from_eps(
        x_t, t_idx, eps_pred, alphas, alpha_bars
    )

    optimizer = torch.optim.Adam(ddpm_cur.parameters(), lr=cfg["finetune"]["lr"])
    log_dir = f"logs/finetune/{time.strftime('%Y%m%d_%H%M')}"
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)

    for epoch in range(1,cfg["finetune"]["num_epochs"]+1):
        stats = single_epoch(
            ddpm_cur=ddpm_cur,
            ddpm_old=ddpm_old,
            rm=rm,
            alphas_cumprod=alpha_bars,
            device=device,
            shape=(1,3,64,64),
            optimizer=optimizer,
            eps_to_mu_fn=eps_to_mu_fn,
            ddim_steps=cfg["finetune"]["ddim_steps"],
            eta=cfg["finetune"]["eta"],
            clip_eps=cfg["finetune"]["clip_eps"],
            episodes_per_epoch=cfg["finetune"]["episodes_per_epoch"],
            grad_clip=cfg["finetune"]["grad_clip"],
            microbatch=cfg["finetune"]["microbatch"]
        )

        # Compute reward of the image generated by recently updated model.
        cur_eval_mean = eval_with_cur_model(ddpm_cur, rm, alpha_bars, device, ddim_steps=cfg["finetune"]["ddim_steps"], eta=cfg["finetune"]["eta"], n_eval=8)

        # Log
        writer.add_scalar('reward/train', stats["reward_mean"], epoch)
        writer.add_scalar('reward/eval', cur_eval_mean, epoch)

        # Set ddpm_old as ddpm_cur and fix it for the next epoch.
        ddpm_old.load_state_dict(ddpm_cur.state_dict())
        ddpm_old.eval()
        for p in ddpm_old.parameters():
            p.requires_grad = False

    writer.close()
    if cfg["finetune"]["save"]:
        torch.save(ddpm_cur.state_dict(), 'ddpm.pth')
        print("Model saved.")

if __name__=="__main__":
    main()